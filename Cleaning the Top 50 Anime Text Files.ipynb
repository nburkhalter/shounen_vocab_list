{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our top 50 shounen anime, and their transcripts downloaded, we need to do some cleaning. In \n",
    "particular, there is going to be a lot of whitespace, timestamps, and text that is not Japanese. To make our NLP \n",
    "analysis easier, we will clean each text file until all that remains is the Japanese.\n",
    "\n",
    "It should be noted before we move further, however, that real-world data is messy and incomplete. In that vein, I \n",
    "must admit that I was unable to find any subtitles for D.Gray Man, Great Teacher Onizuka, Claymore, Katekyo Hitman \n",
    "Reborn, Rosario to Vampire, Watashi Ga Motenai no wa Dou Kangaetemo..., and Kaze no Stigma. Further, I was only able \n",
    "to find incomplete transcripts for Naruto, Soul Eater, One Piece, My Hero Academia, and Pandora Hearts. \n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Though we only have a little less than 86% representation of the top 50 anime present, we also have 4368 text files \n",
    "amassed for analysis. Because our ultimate goal is to find the most common words and phrases amongst the top 50 shounen \n",
    "anime, I believe we can still achieve our goal with this sample set, and posit that had we had 100% representation in \n",
    "our analysis, the results would only be further skewed toward the results we will get with the current sample available.\n",
    "\n",
    "\n",
    "*NOTE*: It is imperative to share that 3 of the top 50 anime (One Piece, Naruto, and Dragon Ball) represent roughly half of all the sample files. This will come into play later when we analyze our bag-of-words results, as each of these\n",
    "anime has its own unique slang and world that may affect our results more than we anticipate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary analysis modules\n",
    "import re\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Our goal for this portion of the project will be to clean each of the 4368 text files and combine them \n",
    "all together to create one giant file full of nothing but lines from the top 50 shounen anime. Once we have this text \n",
    "file, we will then perform a NLP bag-of-words analysis to get our \"ultimate\" vocaulary list.\n",
    "\n",
    "The flow of this workbook is as follows:\n",
    "\n",
    "1. Open text file from computer\n",
    "2. Remove unwanted text, numbers, stop words, and whitespace from said file\n",
    "3. Append cleaned file to a \"global\" text file that will serve as our bag-of-words analysis document\n",
    "4. Rinse and repeat\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "The files for analysis that we got were originally .srt and .ass formatted files. They were converted to .txt for \n",
    "convenience purposes. However, the way these different file extensions are formatted when converted to text are \n",
    "remarkably different. As such, we will make different functions for cleaning both .srt and .ass files.\n",
    "\n",
    ".srt files are easier to deal with, so we'll start with them\n",
    "\n",
    "'''\n",
    "\n",
    "def srt_munging(text_file):\n",
    "    \n",
    "    srt_filepath = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/Subtitle Files By Type/SRT/'\n",
    "    filename = srt_filepath + text_file\n",
    "    \n",
    "    #Open the file, take the text, and create a list\n",
    "    newfile = open(filename, 'rt')\n",
    "    text = newfile.read()\n",
    "    newfile.close()\n",
    "    \n",
    "    lines = text.split()\n",
    "    \n",
    "    \n",
    "    # Regex and string expressions to be called throughout the function\n",
    "    squiggly = '～'\n",
    "    squiggly2 = '〜'\n",
    "    quarter_note =re.compile(r'♪+')\n",
    "    eighth_note = re.compile(r'♬+')\n",
    "    large_open_pointer = '〈'\n",
    "    large_double_open = '《'\n",
    "    large_closed_pointer = '〉'\n",
    "    large_double_closed = '》'\n",
    "    double_open_pointer = '≪'\n",
    "    double_closed_pointer = '≫'\n",
    "    large_thin_open = '＜'\n",
    "    large_thin_closed = '＞'\n",
    "    fat_arrow = '➡'\n",
    "    pesky_arrow = '→'\n",
    "    open_block_quote = '『'\n",
    "    closed_block_quote = '』'\n",
    "    double_paren_open = re.compile(r'\\(\\(')\n",
    "    double_paren_closed = re.compile(r'\\)\\)')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Start working by removing digits\n",
    "    no_digits = []\n",
    "    for i in lines:        \n",
    "        # Remove lines that contain only numbers\n",
    "        if not i.isdigit():\n",
    "            no_digits.append(i)\n",
    "            \n",
    "    \n",
    "    # Remove the '-->' symbol that shows span of time\n",
    "    no_arrows = []          \n",
    "    for i in no_digits:\n",
    "        if not i == '-->':\n",
    "            no_arrows.append(i)\n",
    "                   \n",
    "        \n",
    "    # Remove unicode character at the beginning of the file\n",
    "    no_unicode = []    \n",
    "    for i in no_arrows:\n",
    "        if not i == '\\ufeff1' and not i == '\\ufeff':\n",
    "            no_unicode.append(i)\n",
    "            \n",
    "    \n",
    "    # Create regex to remove timestamps from no_unicode list\n",
    "    timestamp = re.compile(r'[\\d\\d:\\d\\d:\\d\\d,\\d\\d\\d]')\n",
    "    no_timestamp = [i for i in no_unicode if not timestamp.match(i)]\n",
    "    \n",
    "    \n",
    "    # Create new empty list to store our newly cleaned data\n",
    "    no_parentheses = []\n",
    "    for line in no_timestamp:\n",
    "        line = re.sub('（.*）','', line)        # Uses *JAPANESE* punctuation marks!!!    \n",
    "        no_parentheses.append(line)\n",
    "        \n",
    "  \n",
    "    # Get rid of those pesky American parentheses\n",
    "    no_american = []  \n",
    "    for line in no_parentheses:       \n",
    "        line = re.sub('\\(.*\\)','', line)        \n",
    "        no_american.append(line)\n",
    "        \n",
    "    no_brackets = []\n",
    "    for i in no_american:\n",
    "        i = re.sub('\\[.*\\]', '', i)\n",
    "        no_brackets.append(i)\n",
    "            \n",
    "        \n",
    "    # Now it's time to clear the decorative punctuations\n",
    "    \n",
    "    # Remove squiggly lines (～)\n",
    "    no_squiggly = []\n",
    "    for i in no_brackets:\n",
    "        i = re.sub(squiggly, '', i)\n",
    "        no_squiggly.append(i)\n",
    "        \n",
    "    no_squiggly2 = []\n",
    "    for i in no_squiggly:\n",
    "        i = re.sub(squiggly2, '', i)\n",
    "        no_squiggly2.append(i)\n",
    "        \n",
    "        \n",
    "    #Remove musical notes\n",
    "    no_quarter = []\n",
    "    no_eighth = []\n",
    "    for i in no_squiggly2:\n",
    "        i = re.sub(quarter_note, '', i)\n",
    "        no_quarter.append(i)\n",
    "        \n",
    "    for i in no_quarter:\n",
    "        i = re.sub(eighth_note, '', i)\n",
    "        no_eighth.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove all the arrows\n",
    "    no_pesky_arrows = []\n",
    "    no_fat_arrows = []\n",
    "    for i in no_eighth:\n",
    "        i = re.sub(pesky_arrow, '', i)\n",
    "        no_pesky_arrows.append(i)\n",
    "        \n",
    "    for i in no_pesky_arrows:\n",
    "        i = re.sub(fat_arrow, '', i)\n",
    "        no_fat_arrows.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove all pointers (< & > types), quotes, and remaining parentheses\n",
    "    no_large_open_pointer = []\n",
    "    no_large_closed_pointer = []\n",
    "    no_large_double_open = []\n",
    "    no_large_double_closed = []\n",
    "    no_double_open = []\n",
    "    no_double_closed = []\n",
    "    no_open_block = []\n",
    "    no_closed_block = []\n",
    "    no_thin_open = []\n",
    "    no_thin_closed = []\n",
    "    no_double_paren_open = []\n",
    "    no_double_paren_closed = []\n",
    "    \n",
    "    for i in no_fat_arrows:\n",
    "        i = re.sub(large_open_pointer, '', i)\n",
    "        no_large_open_pointer.append(i)\n",
    "        \n",
    "    for i in no_large_open_pointer:\n",
    "        i = re.sub(large_closed_pointer, '', i)\n",
    "        no_large_closed_pointer.append(i)\n",
    "        \n",
    "    for i in no_large_closed_pointer:\n",
    "        i = re.sub(large_double_open, '', i)\n",
    "        no_large_double_open.append(i)\n",
    "        \n",
    "    for i in no_large_double_open:\n",
    "        i = re.sub(large_double_closed, '', i)\n",
    "        no_large_double_closed.append(i)\n",
    "        \n",
    "    for i in no_large_double_closed:\n",
    "        i = re.sub(double_open_pointer, '', i)\n",
    "        no_double_open.append(i)\n",
    "        \n",
    "    for i in no_double_open:\n",
    "        i = re.sub(double_closed_pointer, '', i)\n",
    "        no_double_closed.append(i)\n",
    "        \n",
    "    for i in no_double_closed:\n",
    "        i = re.sub(open_block_quote, '', i)\n",
    "        no_open_block.append(i)\n",
    "        \n",
    "    for i in no_open_block:\n",
    "        i = re.sub(closed_block_quote, '', i)\n",
    "        no_closed_block.append(i)\n",
    "        \n",
    "    for i in no_closed_block:\n",
    "        i = re.sub(large_thin_open, '', i)\n",
    "        no_thin_open.append(i)\n",
    "        \n",
    "    for i in no_thin_open:\n",
    "        i = re.sub(large_thin_closed, '', i)\n",
    "        no_thin_closed.append(i)\n",
    "        \n",
    "    for i in no_thin_closed:\n",
    "        i = re.sub(double_paren_open, '', i)\n",
    "        no_double_paren_open.append(i)\n",
    "        \n",
    "    for i in no_double_paren_open:\n",
    "        i = re.sub(double_paren_closed, '', i)\n",
    "        no_double_paren_closed.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove English characters and punctuations\n",
    "    all_japanese = []\n",
    "    for i in no_double_paren_closed:\n",
    "        if not i == ')' and not i == '(' and not i == '｡':\n",
    "            i = re.sub(r'[A-Za-z]', '', i)\n",
    "            all_japanese.append(i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Exclude empty elements from our list\n",
    "    # This piece of code should ALWAYS be last\n",
    "    full_dialogue = []\n",
    "    for i in all_japanese:\n",
    "        if not i == '':\n",
    "            full_dialogue.append(i)\n",
    "        \n",
    "        \n",
    "    return full_dialogue\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Next, we will write a function to handle the more difficult .ass files. There is a lot going on in these, to \n",
    "include CSS encoding, timestamps, and other kinds of formatting. So this could get messy!'''\n",
    "\n",
    "# In all honesty, I just wanted to make a function called ass_munging\n",
    "\n",
    "def ass_munging(text_file):\n",
    "    \n",
    "    srt_filepath = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/Subtitle Files By Type/ASS/'\n",
    "    filename = srt_filepath + text_file\n",
    "    \n",
    "    #Open the file, take the text, and create a list\n",
    "    newfile = open(filename, 'rt')\n",
    "    text = newfile.read()\n",
    "    newfile.close()\n",
    "    \n",
    "    lines = text.splitlines()\n",
    "    dialogue_box = [i for i in lines if i.startswith('Dialogue')]\n",
    "    \n",
    "    \n",
    "    # Regular & string expressions to be used throughout to help clean the data    \n",
    "    formatted = re.compile(r'[A-Za-z]')\n",
    "    timestamp = re.compile(r'[\\d,\\d:\\d\\d:\\d\\d.\\d\\d,\\d:\\d\\d:\\d\\d.\\d\\d,.*\\,,0000,0000,0000,,]')\n",
    "    digits = re.compile(r'[0-9]')\n",
    "    nichibun = '日文'               # Can't read Chinese, so the variable \"translations\" are probably horribly incorrect\n",
    "    taikatsu = '对话'\n",
    "    useless_chinese1 = '制作人员==诸神字幕组==听译：朝颜'\n",
    "    useless_chinese2 = '警示标语本字幕由诸神字幕组制作仅供学习交流之用若您喜欢本作品请支持正版影像制品'\n",
    "    useless_chinese3 = '制作，仅供交流学习，禁止用于任何商业用途'\n",
    "    useless_chinese4 = '日听兰樱&'\n",
    "    useless_chinese5 = '本字幕由诸神字幕组出品'\n",
    "    useless_chinese6 = '翻译：'\n",
    "    useless_chinese7 = '諸神字幕組本字幕由诸神字幕组制作仅供学习交流之用若您喜欢本作品请支持正版影像制品'\n",
    "    useless_chinese8 = '諸神字幕組{制作仅供学习交流之用若您喜欢本作品请支持正版影像制品'\n",
    "    useless_chinese9 = '諸神字幕組'\n",
    "    squiggly = '～'\n",
    "    dash = re.compile(r'[-]+')\n",
    "    brackets = re.compile(r'\\{.*\\}')\n",
    "    white_noise_apos = re.compile(r'(`)+')\n",
    "    white_noise_dot = re.compile(r'(·)+')\n",
    "    white_noise_dot2 = re.compile(r'(・)+')\n",
    "    pesky_arrow = '→'\n",
    "    open_pointer = '≪'\n",
    "    closed_pointer = '≫'\n",
    "    large_japanese_open_pointer = '《'\n",
    "    large_japanese_closed_pointer = '》'\n",
    "    another_open_pointer = '＜'\n",
    "    equals_sign = re.compile(r'[=]+')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Remove that pesky unicode\n",
    "    no_unicode = []\n",
    "    for i in dialogue_box:\n",
    "        if not i == '\\ufeff1' and not i == '\\ufeff':\n",
    "            no_unicode.append(i)\n",
    "            \n",
    "    no_spaces = []\n",
    "    for i in no_unicode:\n",
    "        i = re.sub('\\u3000', '', i)\n",
    "        no_spaces.append(i)\n",
    "        \n",
    "    extra_spaces = []\n",
    "    for i in no_spaces:\n",
    "        i = re.sub(' ', '', i)\n",
    "        extra_spaces.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove tab characters\n",
    "    no_tabs = []\n",
    "    for i in extra_spaces:\n",
    "        i = re.sub('\\t', '', i)\n",
    "        no_tabs.append(i)\n",
    "    \n",
    "    \n",
    "    # Removes formatting lines\n",
    "    no_formatting = []\n",
    "    for i in no_tabs:\n",
    "        i = re.sub(timestamp, '', i)\n",
    "        no_formatting.append(i)\n",
    "    \n",
    "    \n",
    "    # Remove the timestamps from the file\n",
    "    no_timestamps = []\n",
    "    for i in no_formatting:\n",
    "        i = re.sub(timestamp,'', i)\n",
    "        no_timestamps.append(i)\n",
    "                              \n",
    "            \n",
    "    # Remove any English-language stragglers\n",
    "    no_english = []\n",
    "    for i in no_timestamps:\n",
    "        i = re.sub(formatted,'', i)\n",
    "        no_english.append(i)\n",
    "        \n",
    "    \n",
    "    # Remove chinese characters from dialogue\n",
    "    no_nichibun = []\n",
    "    for i in no_english:\n",
    "        i = re.sub(nichibun, '', i)\n",
    "        no_nichibun.append(i)\n",
    "        \n",
    "    no_taikatsu = []\n",
    "    for i in no_nichibun:\n",
    "        i = re.sub(taikatsu, '', i)\n",
    "        no_taikatsu.append(i)\n",
    "                           \n",
    "    \n",
    "    # Remove backslashes from text\n",
    "    no_backslashes = []\n",
    "    for i in no_taikatsu:\n",
    "        i = re.sub(r'\\\\', '', i)\n",
    "        no_backslashes.append(i)\n",
    "        \n",
    "        \n",
    "    # Loop through the last list to remove the parenthetical phrases\n",
    "    no_parentheses = []\n",
    "    for line in no_backslashes:\n",
    "        line = re.sub('（.*）','', line)        # Uses *JAPANESE* punctuation marks!!!    \n",
    "        no_parentheses.append(line)\n",
    "        \n",
    "    \n",
    "    # Get rid of those pesky American parentheses\n",
    "    no_american = []  \n",
    "    for line in no_parentheses:        \n",
    "        line = re.sub('\\(.*\\)','', line)        \n",
    "        no_american.append(line)\n",
    "        \n",
    "        \n",
    "    # Remove dashes (hyphens) from text\n",
    "    no_hyphens = []\n",
    "    for i in no_american:\n",
    "        i = re.sub(dash, '', i)\n",
    "        no_hyphens.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove equals signs\n",
    "    no_equals = []\n",
    "    for i in no_hyphens:\n",
    "        i = re.sub(equals_sign, '', i)\n",
    "        no_equals.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove squiggly (～)\n",
    "    no_squiggly = []\n",
    "    for i in no_equals:\n",
    "        i = re.sub(squiggly, '', i)\n",
    "        no_squiggly.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove brackets and their content from string element\n",
    "    no_brackets = []\n",
    "    for i in no_squiggly:\n",
    "        i = re.sub(brackets, '', i)\n",
    "        no_brackets.append(i)\n",
    "        \n",
    "    \n",
    "    # Further remove any straggler brackets and quotation marks from our data (open brackets)\n",
    "    no_open_brackets = []\n",
    "    for i in no_brackets:\n",
    "        if not i == '{' and not i == '“':\n",
    "            no_open_brackets.append(i)\n",
    "            \n",
    "            \n",
    "    # Remove any random lines of Japanese punctuation, pt.1\n",
    "    noise_apos = []\n",
    "    for i in no_open_brackets:\n",
    "        i = re.sub(white_noise_apos, '', i)\n",
    "        noise_apos.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove any random lines of Japanese punctuation, pt. 2\n",
    "    noise_dot = []\n",
    "    for i in noise_apos:\n",
    "        i = re.sub(white_noise_dot, '', i)\n",
    "        noise_dot.append(i)\n",
    "        \n",
    "    # Chinese and Japanese dots are different...\n",
    "    noise_dot2 = []\n",
    "    for j in noise_dot:\n",
    "        j = re.sub(white_noise_dot2, '', j)\n",
    "        noise_dot2.append(j)\n",
    "        \n",
    "        \n",
    "    # Remove arrow symbols\n",
    "    no_arrows = []\n",
    "    for i in noise_dot2:\n",
    "        i = re.sub(pesky_arrow, '', i)\n",
    "        no_arrows.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove pointers (≪ & ≫)\n",
    "    no_open_pointers = []\n",
    "    for i in no_arrows:\n",
    "        i = re.sub(open_pointer, '', i)\n",
    "        no_open_pointers.append(i)\n",
    "        \n",
    "    no_closed_pointers = []\n",
    "    for i in no_open_pointers:\n",
    "        i = re.sub(closed_pointer, '', i)\n",
    "        no_closed_pointers.append(i)\n",
    "        \n",
    "    # Just like dots, Chinese and Japanese pointers are different...\n",
    "    no_large_open_pointers = []\n",
    "    for i in no_closed_pointers:\n",
    "        i = re.sub(large_japanese_open_pointer, '', i)\n",
    "        no_large_open_pointers.append(i)\n",
    "        \n",
    "    no_large_closed_pointers = []\n",
    "    for i in no_large_open_pointers:\n",
    "        i = re.sub(large_japanese_closed_pointer, '', i)\n",
    "        no_large_closed_pointers.append(i)\n",
    "        \n",
    "    # But wait, there's one more pointer!\n",
    "    plz_no_more_pointers = []\n",
    "    for i in no_large_closed_pointers:\n",
    "        i = re.sub(another_open_pointer, '', i)\n",
    "        plz_no_more_pointers.append(i)\n",
    "        \n",
    "        \n",
    "    # Last bit of cleanup concerning random punctuation and Chinese\n",
    "    language_only = [i for i in plz_no_more_pointers if not '『' in i]\n",
    "    no_Chinese = [i for i in language_only if not '制作人员' in i and not '标题' in i]\n",
    "    \n",
    "    \n",
    "    # Now let's get rid of that random Chinese text still scattered about our files\n",
    "    clean_text = []\n",
    "    for i in no_Chinese:\n",
    "        if not i == useless_chinese1 and not i == useless_chinese2 and not i == useless_chinese3 and not i == useless_chinese4 and not i == useless_chinese5 and not i == useless_chinese6 and not i == useless_chinese7 and not i == useless_chinese8 and not i == useless_chinese9 and not i == '－－':\n",
    "            clean_text.append(i)\n",
    "        \n",
    "    no_chuubun = []\n",
    "    for i in clean_text:\n",
    "        if not i.startswith('中文'):\n",
    "            no_chuubun.append(i)\n",
    "    \n",
    "    \n",
    "    # Exclude empty elements from our list\n",
    "    # This piece of code should ALWAYS be last\n",
    "    full_dialogue = []\n",
    "    for i in no_chuubun:\n",
    "        if not i == '':\n",
    "            full_dialogue.append(i)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    return full_dialogue\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''When analyzing the .ass files, it was noticed that a few of them were encoded, thus could not be processed like \n",
    "the rest of the data. In order to solve that, we have 2 options:\n",
    "\n",
    "1. See if we can simply remove the encoded lines from the files directly (there aren't that many)\n",
    "2. Write some more code that decodes the files, performs the cleaning, then re-encodes the file\n",
    "\n",
    "We will start with option 1. Ichiban Ushiro no Daimaou, Fullmetal Alchemist: Brotherhood, and a handful of Gintama \n",
    "episodes were the culprits. If we were dealing with thousands of encoded files, we would jump right ahead to option 2, \n",
    "but since the remaining files total less than 75, it may be easier just to make the individual changes.\n",
    "\n",
    "I have organized the files into a new sub-directory labeled \"Encoded ASS\". We will essentially copy the ass_munging \n",
    "function above, change the file path, delete a couple lines from the original file (none with Japanese text) and see \n",
    "if that works. If so, this section is all but done. If not, we simply move on to option 2.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "UPDATE: The files in question were coded in utf-16 (apparently pretty common amongst Asian languages). We will move to \n",
    "option 2, but the fix is incredibly simple. When we open the file, we will specify the keyword argument \n",
    "*encoding='utf-16'*. This will allow us to perform the necessary cleaning like before.\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def encoded_munging(text_file):\n",
    "    \n",
    "    srt_filepath = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/Subtitle Files By Type/Encoded ASS/'\n",
    "    filename = srt_filepath + text_file\n",
    "    \n",
    "    #Open the file, take the text, and create a list\n",
    "    newfile = open(filename, 'rb')        # Open file to read in bytes\n",
    "    text = newfile.read()\n",
    "    \n",
    "    bom = codecs.BOM_UTF16_LE             # Print dir(codecs) for other encodings\n",
    "    if text.startswith(bom):              # Make sure the encoding is what you expect, otherwise you'll get wrong data\n",
    "        encoded_text = text[len(bom):]    # Strip away the BOM\n",
    "        decoded_text = encoded_text.decode('utf-16le')  # Decode to unicode\n",
    "    else:\n",
    "        decoded_text = text.decode('utf-8')\n",
    "    \n",
    "    newfile.close()\n",
    "    \n",
    "    lines = decoded_text.split()\n",
    "    \n",
    "    \n",
    "    # Regular & string expressions to be used throughout to help clean the data    \n",
    "    formatted = re.compile(r'[A-Za-z]')\n",
    "    timestamp = re.compile(r'[\\d,\\d:\\d\\d:\\d\\d.\\d\\d,\\d:\\d\\d:\\d\\d.\\d\\d,.*\\,,0000,0000,0000,,]')\n",
    "    digits = re.compile(r'[0-9]')\n",
    "    nichibun = '日文'         # Can't read Chinese, so the variable \"translations\" are probably horribly incorrect\n",
    "    taikatsu = '对话'\n",
    "    useless_chinese1 = '制作人员==诸神字幕组==听译：朝颜'\n",
    "    useless_chinese2 = '警示标语本字幕由诸神字幕组制作仅供学习交流之用若您喜欢本作品请支持正版影像制品'\n",
    "    useless_chinese3 = '制作，仅供交流学习，禁止用于任何商业用途'\n",
    "    useless_chinese4 = '日听兰樱&'\n",
    "    useless_chinese5 = '本字幕由诸神字幕组出品'\n",
    "    useless_chinese6 = '翻译：'\n",
    "    useless_chinese7 = '諸神字幕組本字幕由诸神字幕组制作仅供学习交流之用若您喜欢本作品请支持正版影像制品'\n",
    "    useless_chinese8 = '諸神字幕組{制作仅供学习交流之用若您喜欢本作品请支持正版影像制品'\n",
    "    useless_chinese9 = '諸神字幕組'\n",
    "    squiggly = '～'\n",
    "    dash = re.compile(r'[-]+')\n",
    "    brackets = re.compile(r'\\{.*\\}')\n",
    "    white_noise_apos = re.compile(r'(`)+')\n",
    "    white_noise_dot = re.compile(r'(·)+')\n",
    "    white_noise_dot2 = re.compile(r'(・)+')\n",
    "    pesky_arrow = '→'\n",
    "    open_pointer = '≪'\n",
    "    closed_pointer = '≫'\n",
    "    large_japanese_open_pointer = '《'\n",
    "    large_japanese_closed_pointer = '》'\n",
    "    another_open_pointer = '＜'\n",
    "    equals_sign = re.compile(r'[=]+')\n",
    "    \n",
    "    \n",
    "                           \n",
    "    # Remove that pesky unicode\n",
    "    no_unicode = []\n",
    "    for i in lines:\n",
    "        if not i == '\\ufeff1' and not i == '\\ufeff':\n",
    "            no_unicode.append(i)\n",
    "            \n",
    "    no_sixteen = []\n",
    "    for i in no_unicode:\n",
    "        i = re.sub('\\ue4c6', '', i)\n",
    "        no_sixteen.append(i)\n",
    "    \n",
    "    \n",
    "    # Removes formatting lines\n",
    "    no_formatting = []\n",
    "    for i in no_sixteen:\n",
    "        i = re.sub(timestamp, '', i)\n",
    "        no_formatting.append(i)\n",
    "    \n",
    "    \n",
    "    # Remove the timestamps from the file\n",
    "    no_timestamps = []\n",
    "    for i in no_formatting:\n",
    "        i = re.sub(timestamp,'', i)\n",
    "        no_timestamps.append(i)\n",
    "                              \n",
    "            \n",
    "    # Remove any English-language stragglers\n",
    "    no_english = []\n",
    "    for i in no_timestamps:\n",
    "        i = re.sub(formatted,'', i)\n",
    "        no_english.append(i)\n",
    "        \n",
    "    \n",
    "    # Remove chinese characters from dialogue\n",
    "    no_nichibun = []\n",
    "    for i in no_english:\n",
    "        i = re.sub(nichibun, '', i)\n",
    "        no_nichibun.append(i)\n",
    "        \n",
    "    no_taikatsu = []\n",
    "    for i in no_nichibun:\n",
    "        i = re.sub(taikatsu, '', i)\n",
    "        no_taikatsu.append(i)\n",
    "                           \n",
    "    \n",
    "    # Remove backslashes from text\n",
    "    no_backslashes = []\n",
    "    for i in no_taikatsu:\n",
    "        i = re.sub(r'\\\\', '', i)\n",
    "        no_backslashes.append(i)\n",
    "        \n",
    "        \n",
    "    # Loop through the last list to remove the parenthetical phrases\n",
    "    no_parentheses = []\n",
    "    for line in no_backslashes:\n",
    "        line = re.sub('（.*）','', line)        # Uses *JAPANESE* punctuation marks!!!    \n",
    "        no_parentheses.append(line)\n",
    "        \n",
    "    \n",
    "    # Get rid of those pesky American parentheses\n",
    "    no_american = []  \n",
    "    for line in no_parentheses:        \n",
    "        line = re.sub('\\(.*\\)','', line)        \n",
    "        no_american.append(line)\n",
    "        \n",
    "        \n",
    "    # Remove dashes (hyphens) from text\n",
    "    no_hyphens = []\n",
    "    for i in no_american:\n",
    "        i = re.sub(dash, '', i)\n",
    "        no_hyphens.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove equals signs\n",
    "    no_equals = []\n",
    "    for i in no_hyphens:\n",
    "        i = re.sub(equals_sign, '', i)\n",
    "        no_equals.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove squiggly (～)\n",
    "    no_squiggly = []\n",
    "    for i in no_equals:\n",
    "        i = re.sub(squiggly, '', i)\n",
    "        no_squiggly.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove brackets and their content from string element\n",
    "    no_brackets = []\n",
    "    for i in no_squiggly:\n",
    "        i = re.sub(brackets, '', i)\n",
    "        no_brackets.append(i)\n",
    "        \n",
    "    \n",
    "    # Further remove any straggler brackets and quotation marks from our data (open brackets)\n",
    "    no_open_brackets = []\n",
    "    for i in no_brackets:\n",
    "        if not i == '{' and not i == '“':\n",
    "            no_open_brackets.append(i)\n",
    "            \n",
    "            \n",
    "    # Remove any random lines of Japanese punctuation, pt.1\n",
    "    noise_apos = []\n",
    "    for i in no_open_brackets:\n",
    "        i = re.sub(white_noise_apos, '', i)\n",
    "        noise_apos.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove any random lines of Japanese punctuation, pt. 2\n",
    "    noise_dot = []\n",
    "    for i in noise_apos:\n",
    "        i = re.sub(white_noise_dot, '', i)\n",
    "        noise_dot.append(i)\n",
    "        \n",
    "    # Chinese and Japanese dots are different...\n",
    "    noise_dot2 = []\n",
    "    for j in noise_dot:\n",
    "        j = re.sub(white_noise_dot2, '', j)\n",
    "        noise_dot2.append(j)\n",
    "        \n",
    "        \n",
    "    # Remove arrow symbols\n",
    "    no_arrows = []\n",
    "    for i in noise_dot2:\n",
    "        i = re.sub(pesky_arrow, '', i)\n",
    "        no_arrows.append(i)\n",
    "        \n",
    "        \n",
    "    # Remove pointers (≪ & ≫)\n",
    "    no_open_pointers = []\n",
    "    for i in no_arrows:\n",
    "        i = re.sub(open_pointer, '', i)\n",
    "        no_open_pointers.append(i)\n",
    "        \n",
    "    no_closed_pointers = []\n",
    "    for i in no_open_pointers:\n",
    "        i = re.sub(closed_pointer, '', i)\n",
    "        no_closed_pointers.append(i)\n",
    "        \n",
    "    # Just like dots, Chinese and Japanese pointers are different...\n",
    "    no_large_open_pointers = []\n",
    "    for i in no_closed_pointers:\n",
    "        i = re.sub(large_japanese_open_pointer, '', i)\n",
    "        no_large_open_pointers.append(i)\n",
    "        \n",
    "    no_large_closed_pointers = []\n",
    "    for i in no_large_open_pointers:\n",
    "        i = re.sub(large_japanese_closed_pointer, '', i)\n",
    "        no_large_closed_pointers.append(i)\n",
    "        \n",
    "    # But wait, there's one more pointer!\n",
    "    plz_no_more_pointers = []\n",
    "    for i in no_large_closed_pointers:\n",
    "        i = re.sub(another_open_pointer, '', i)\n",
    "        plz_no_more_pointers.append(i)\n",
    "        \n",
    "        \n",
    "    # Last bit of cleanup concerning random punctuation and Chinese\n",
    "    language_only = [i for i in plz_no_more_pointers if not '『' in i]\n",
    "    no_Chinese = [i for i in language_only if not '制作人员' in i and not '标题' in i]\n",
    "    \n",
    "    \n",
    "    # Now let's get rid of that random Chinese text still scattered about our files\n",
    "    clean_text = []\n",
    "    for i in no_Chinese:\n",
    "        if not i == useless_chinese1 and not i == useless_chinese2 and not i == useless_chinese3 and not i == useless_chinese4 and not i == useless_chinese5 and not i == useless_chinese6 and not i == useless_chinese7 and not i == useless_chinese8 and not i == useless_chinese9 and not i == '－－' and not i == '//':\n",
    "            clean_text.append(i)\n",
    "        \n",
    "    no_chuubun = []\n",
    "    for i in clean_text:\n",
    "        if not i.startswith('中文'):\n",
    "            no_chuubun.append(i)\n",
    "    \n",
    "    \n",
    "    # Exclude empty elements from our list\n",
    "    # This piece of code should ALWAYS be last\n",
    "    full_dialogue = []\n",
    "    for i in no_chuubun:\n",
    "        if not i == '':\n",
    "            full_dialogue.append(i)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "    return full_dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Great! Now that the files have been sufficiently munged, we can write all the .srt and .ass files to their own \n",
    "master files (corpora), then combine them to create one massive anime corpus!'''\n",
    "\n",
    "srt_corpus = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/Corpora/SRT_Corpus.txt'\n",
    "ass_corpus = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/Corpora/ASS_Corpus.txt'\n",
    "encoded_corpus = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/Corpora/ENCODED_Corpus.txt'\n",
    "\n",
    "\n",
    "# SRT preparation - list of all SRT-based files\n",
    "srt_consolidated = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/consolidated_srt.txt'\n",
    "srt_list = open(srt_consolidated)\n",
    "consolidated_list = srt_list.readlines()\n",
    "srt_list.close()\n",
    "\n",
    "srt_anime = []\n",
    "for i in consolidated_list:\n",
    "    srt_anime.append(i.strip())\n",
    "    \n",
    "    \n",
    "# ASS preparation - list of all ASS_based files\n",
    "ass_consolidated = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/consolidated_ass.txt'\n",
    "ass_list = open(ass_consolidated)\n",
    "consolidated_ass = ass_list.readlines()\n",
    "ass_list.close()\n",
    "\n",
    "ass_anime = []\n",
    "for i in consolidated_ass:\n",
    "    ass_anime.append(i.strip())\n",
    "    \n",
    "    \n",
    "# ENCODED preparation - list of all ENCODED files\n",
    "encoded_consolidated = '/Users/nickburkhalter/Desktop/Data Science/Projects/Top Shounen Anime Vocab List/consolidated_encoded.txt'\n",
    "encoded_list = open(encoded_consolidated)\n",
    "consolidated_encoded = encoded_list.readlines()\n",
    "encoded_list.close()\n",
    "\n",
    "encoded_anime = []\n",
    "for i in consolidated_encoded:\n",
    "    encoded_anime.append(i.strip())\n",
    "\n",
    "\n",
    "    \n",
    "# Write program that opens corpus file, appends each transcript line to the corpus, then closes the file\n",
    "with open(srt_corpus, 'a') as srt:\n",
    "    for i in srt_anime:\n",
    "        srt.write(str(srt_munging(i)) + '\\n')\n",
    "        \n",
    "with open(ass_corpus, 'a') as ass:\n",
    "    for i in ass_anime:\n",
    "        ass.write(str(ass_munging(i)) + '\\n')\n",
    "        \n",
    "with open(encoded_corpus, 'a') as enc:\n",
    "    for i in encoded_anime:\n",
    "        enc.write(str(encoded_munging(i)) + '\\n')\n",
    "        \n",
    "        \n",
    "# To save headaches, since we were dealing with both utf-8 and utf-16 file types for the encoded_munging function,\n",
    "# we simply save the written-to file as utf-8\n",
    "\n",
    "# Lastly, we use the command line to merge all 3 files to create our ultimate anime corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
